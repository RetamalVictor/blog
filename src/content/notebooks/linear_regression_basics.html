<div class="blog-content">
    <p class="lead">Linear regression is one of the fundamental algorithms in machine learning and statistics. In this post, we'll explore the mathematical foundations and implement it from scratch using NumPy.</p>

    <h2>Mathematical Foundation</h2>
    <p>Linear regression assumes a linear relationship between input features <em>X</em> and target variable <em>y</em>:</p>
    <div class="math-block">
        $$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n + \epsilon$$
    </div>

    <p>In matrix form: <em>y = Xβ + ε</em></p>

    <p>Where:</p>
    <ul>
        <li><strong>β</strong> are the parameters we want to learn</li>
        <li><strong>ε</strong> represents the error term</li>
        <li>Our goal is to minimize the sum of squared errors (SSE)</li>
    </ul>

    <h2>Implementation from Scratch</h2>
    <p>The normal equation for linear regression is:</p>
    <div class="math-block">
        $$\beta = (X^T X)^{-1} X^T y$$
    </div>

    <p>This gives us the optimal parameters that minimize the mean squared error.</p>

    <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression

class LinearRegressionFromScratch:
    def __init__(self):
        self.weights = None
        self.bias = None

    def fit(self, X, y):
        """Fit the model using the normal equation"""
        # Add bias term (intercept) by adding a column of ones
        X_with_bias = np.c_[np.ones(X.shape[0]), X]

        # Normal equation: β = (X^T X)^(-1) X^T y
        params = np.linalg.inv(X_with_bias.T @ X_with_bias) @ X_with_bias.T @ y

        self.bias = params[0]
        self.weights = params[1:]

        return self

    def predict(self, X):
        """Make predictions"""
        return self.bias + X @ self.weights

    def mse(self, X, y):
        """Calculate Mean Squared Error"""
        predictions = self.predict(X)
        return np.mean((y - predictions) ** 2)

    def r_squared(self, X, y):
        """Calculate R-squared (coefficient of determination)"""
        predictions = self.predict(X)
        ss_res = np.sum((y - predictions) ** 2)  # Sum of squared residuals
        ss_tot = np.sum((y - np.mean(y)) ** 2)   # Total sum of squares
        return 1 - (ss_res / ss_tot)

# Generate synthetic data
X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)

# Create and train the model
model = LinearRegressionFromScratch()
model.fit(X, y)

print(f"Learned parameters:")
print(f"Bias (β₀): {model.bias:.2f}")
print(f"Weight (β₁): {model.weights[0]:.2f}")
</code></pre>

    <h2>Model Evaluation</h2>
    <p>Let's evaluate our implementation:</p>

    <pre><code class="language-python"># Make predictions and calculate metrics
predictions = model.predict(X)
mse = model.mse(X, y)
r2 = model.r_squared(X, y)

print(f"Mean Squared Error: {mse:.2f}")
print(f"R-squared Score: {r2:.3f}")

# Visualize results
plt.figure(figsize=(10, 6))
plt.scatter(X, y, alpha=0.7, label='Data points')
plt.plot(X, predictions, color='red', linewidth=2, label='Regression line')
plt.xlabel('Feature (X)')
plt.ylabel('Target (y)')
plt.title('Linear Regression Results')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
</code></pre>

    <h2>Key Takeaways</h2>

    <h3>Linear Regression Assumptions</h3>
    <ul>
        <li>Linear relationship between features and target</li>
        <li>Independence of observations</li>
        <li>Homoscedasticity (constant variance of errors)</li>
        <li>Normality of residuals</li>
    </ul>

    <h3>Normal Equation vs Gradient Descent</h3>
    <ul>
        <li><strong>Normal equation</strong>: Exact solution, but computationally expensive for large datasets O(n³)</li>
        <li><strong>Gradient descent</strong>: Iterative approach, scales better to large datasets</li>
    </ul>

    <h3>Performance Metrics</h3>
    <ul>
        <li><strong>MSE</strong>: Lower is better, sensitive to outliers</li>
        <li><strong>R²</strong>: Proportion of variance explained, ranges from 0 to 1</li>
    </ul>

    <h3>When to Use Linear Regression</h3>
    <ul>
        <li>When you need interpretable models</li>
        <li>As a baseline for more complex models</li>
        <li>When the relationship is approximately linear</li>
    </ul>

    <p>This implementation demonstrates the core concepts of linear regression and provides a foundation for understanding more advanced machine learning algorithms!</p>
</div>