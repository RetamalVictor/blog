{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Regression: From Theory to Implementation\n",
        "\n",
        "Linear regression is one of the fundamental algorithms in machine learning and statistics. In this notebook, we'll explore the mathematical foundations and implement it from scratch using NumPy.\n",
        "\n",
        "## Mathematical Foundation\n",
        "\n",
        "Linear regression assumes a linear relationship between input features $X$ and target variable $y$:\n",
        "\n",
        "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\epsilon$$\n",
        "\n",
        "In matrix form: $y = X\\beta + \\epsilon$\n",
        "\n",
        "Where:\n",
        "- $\\beta$ are the parameters we want to learn\n",
        "- $\\epsilon$ represents the error term\n",
        "- Our goal is to minimize the sum of squared errors (SSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Sample Data\n",
        "\n",
        "Let's create a simple dataset to work with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic data\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Test data shape: {X_test.shape}\")\n",
        "\n",
        "# Visualize the data\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_train, y_train, alpha=0.7, label='Training data')\n",
        "plt.scatter(X_test, y_test, alpha=0.7, label='Test data', color='red')\n",
        "plt.xlabel('Feature (X)')\n",
        "plt.ylabel('Target (y)')\n",
        "plt.title('Linear Regression Dataset')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implement Linear Regression from Scratch\n",
        "\n",
        "The normal equation for linear regression is:\n",
        "\n",
        "$$\\beta = (X^T X)^{-1} X^T y$$\n",
        "\n",
        "This gives us the optimal parameters that minimize the mean squared error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinearRegressionFromScratch:\n",
        "    def __init__(self):\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit the model using the normal equation\"\"\"\n",
        "        # Add bias term (intercept) by adding a column of ones\n",
        "        X_with_bias = np.c_[np.ones(X.shape[0]), X]\n",
        "        \n",
        "        # Normal equation: β = (X^T X)^(-1) X^T y\n",
        "        params = np.linalg.inv(X_with_bias.T @ X_with_bias) @ X_with_bias.T @ y\n",
        "        \n",
        "        self.bias = params[0]\n",
        "        self.weights = params[1:]\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        return self.bias + X @ self.weights\n",
        "    \n",
        "    def mse(self, X, y):\n",
        "        \"\"\"Calculate Mean Squared Error\"\"\"\n",
        "        predictions = self.predict(X)\n",
        "        return np.mean((y - predictions) ** 2)\n",
        "    \n",
        "    def r_squared(self, X, y):\n",
        "        \"\"\"Calculate R-squared (coefficient of determination)\"\"\"\n",
        "        predictions = self.predict(X)\n",
        "        ss_res = np.sum((y - predictions) ** 2)  # Sum of squared residuals\n",
        "        ss_tot = np.sum((y - np.mean(y)) ** 2)   # Total sum of squares\n",
        "        return 1 - (ss_res / ss_tot)\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegressionFromScratch()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Learned parameters:\")\n",
        "print(f\"Bias (β₀): {model.bias:.2f}\")\n",
        "print(f\"Weight (β₁): {model.weights[0]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "train_predictions = model.predict(X_train)\n",
        "test_predictions = model.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "train_mse = model.mse(X_train, y_train)\n",
        "test_mse = model.mse(X_test, y_test)\n",
        "train_r2 = model.r_squared(X_train, y_train)\n",
        "test_r2 = model.r_squared(X_test, y_test)\n",
        "\n",
        "print(\"Model Performance:\")\n",
        "print(f\"Training MSE: {train_mse:.2f}\")\n",
        "print(f\"Test MSE: {test_mse:.2f}\")\n",
        "print(f\"Training R²: {train_r2:.3f}\")\n",
        "print(f\"Test R²: {test_r2:.3f}\")\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Training data with regression line\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_train, y_train, alpha=0.7, label='Training data')\n",
        "X_line = np.linspace(X_train.min(), X_train.max(), 100).reshape(-1, 1)\n",
        "y_line = model.predict(X_line)\n",
        "plt.plot(X_line, y_line, color='red', linewidth=2, label=f'Regression line (R² = {train_r2:.3f})')\n",
        "plt.xlabel('Feature (X)')\n",
        "plt.ylabel('Target (y)')\n",
        "plt.title('Training Data with Regression Line')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Predictions vs Actual\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(y_test, test_predictions, alpha=0.7)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Predictions vs Actual (Test Set)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Linear Regression Assumptions**:\n",
        "   - Linear relationship between features and target\n",
        "   - Independence of observations\n",
        "   - Homoscedasticity (constant variance of errors)\n",
        "   - Normality of residuals\n",
        "\n",
        "2. **Normal Equation vs Gradient Descent**:\n",
        "   - Normal equation: Exact solution, but computationally expensive for large datasets O(n³)\n",
        "   - Gradient descent: Iterative approach, scales better to large datasets\n",
        "\n",
        "3. **Performance Metrics**:\n",
        "   - **MSE**: Lower is better, sensitive to outliers\n",
        "   - **R²**: Proportion of variance explained, ranges from 0 to 1\n",
        "\n",
        "4. **When to Use Linear Regression**:\n",
        "   - When you need interpretable models\n",
        "   - As a baseline for more complex models\n",
        "   - When the relationship is approximately linear\n",
        "\n",
        "This implementation demonstrates the core concepts of linear regression and provides a foundation for understanding more advanced machine learning algorithms!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}